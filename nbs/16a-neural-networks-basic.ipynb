{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks 101 - 2 Layers Deep Neural Networks from Scratch\n",
    "\n",
    "สอนสร้าง นิวรอลเน็ตเวิร์ก ทีละขั้นตอน แบบเข้าใจง่าย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.core.debugger import set_trace\n",
    "from fastai import datasets\n",
    "import pickle, gzip, math, torch\n",
    "from torch import tensor\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ประกาศฟังก์ชันที่ใช้ในการเทส ค่าใกล้เคียงกัน และค่าใกล้ 0 เนื่องจาก ตัวเลขแบบ Floating Point จะไม่สามารถเปรียบเทียบว่าเท่ากันหรือไม่ได้ตรง ๆ เพราะจะมีความคลาดเคลื่อนอยู่ โดยเฉพาะอย่างยิ่งเลขมาก ๆ และเลขน้อย ๆ เช่น 123456789.0 หรือ 0.123456789 แต่เราจะใช้ค่า tol (tolerance) ว่าถ้าห่างกันไม่เกินค่านี้ ก็ถือว่ามีค่าเท่ากัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tol = tolerance\n",
    "def test(a,b,cmp,cname=None):\n",
    "    if cname is None: cname=cmp.__name__\n",
    "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
    "\n",
    "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
    "def test_near(a,b): test(a,b,near)    \n",
    "    \n",
    "def test_near_zero(a, tol=1e-3): assert a.abs() < tol, f\"Near Zero: {a}\"    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เคสนี้เราจะใช้ MNIST Dataset เหมือนเดิม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
    "    with gzip.open(path, 'rb') as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "    return map(tensor, (x_train, y_train, x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ดาวน์โหลดชุดข้อมูลมาใส่ x, y ทั้ง Training Set และ Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. เตรียมข้อมูล"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หา mean และ standard deviation ของข้อมูล Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean, train_std = x_train.mean(), x_train.std()\n",
    "train_mean, train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize คือการทำให้ข้อมูล มี mean = 0 และ standard deviation (std) = 1 ด้วยกัน ลบด้วย mean หารด้วย std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = data, m = mean, s = standard deviation\n",
    "def normalize(x, m, s): \n",
    "    return (x-m)/s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = normalize(x_train, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize ข้อมูล Validation Set ด้วย mean, std ของ Training Set เพื่อปรับให้เป็น Scale เดียวกัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = normalize(x_valid, train_mean, train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ลองดูค่า mean, std หลังจาก Normalize เรียบร้อยแล้ว"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-7.6999e-06), tensor(1.))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mean_after, train_std_after = x_train.mean(), x_train.std()\n",
    "train_mean_after, train_std_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ได้ mean เป็นจุดทศนิยมน้อยมาก ๆ ใกล้เคียง 0 และ std ใกล้เคียง 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near_zero(train_mean_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near_zero(1-train_std_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n = จำนวน Record, m จำนวน Feature, c = จำนวน class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะสร้าง 2 Hidden Layers Neural Networks ที่มี 50 Neuron ในแต่ละ Hidden Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nh = number of hidden\n",
    "nh = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "กำหนดค่าเริ่มต้นของ Weight และ Bias ด้วยค่า Random แบบ Normal Distribution (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.randn(m, nh)\n",
    "b1 = torch.zeros(nh)\n",
    "w2 = torch.randn(nh, 1)\n",
    "b2 = torch.zeros(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ลองดูค่า mean, std ของ Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0013), tensor(1.0006))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = data, w = weight, b = bias\n",
    "def lin(x, w, b): \n",
    "    return x@w + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x.clamp_min(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb = x batch, l2 = layer 2, a1 = activation 1\n",
    "def model(xb):\n",
    "    l0 = xb\n",
    "    l1 = lin(l0, w1, b1)\n",
    "    a1 = relu(l1)\n",
    "    l2 = lin(a1, w2, b2)    \n",
    "    return l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yhat (y hat) คือ y ที่ได้จากการคำนวนของโมเดล ที่เราจะเอาไปเปรียบเทียบกับ y จริง ๆ จาก Training Set หรือ Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), torch.Size([50000]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ในที่นี้เราจะใช้ Mean Squared Error แทน Cross Entropy Loss เพื่อลดความซับซ้อนของโค้ดให้เข้าใจง่ายขึ้น (แต่จริง ๆ ไม่เหมาะกับงาน Classification แบบนี้)\n",
    "\n",
    "*จาก shape ด้านบน yhat เป็น tensor 2 มิติ เราจะ squeeze มิติสุดท้าย (-1) ทิ้งไป ก่อนลบกับ y*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(yhat, y): return ((yhat.squeeze(-1)-y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "แปลงจาก Long เป็น Float ก่อนส่งให้ฟังก์ชัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_valid = y_train.float(), y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46558.8945)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(yhat, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Backpropagation and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ประกาศฟังก์ชัน Diff ของฟังก์ชันด้านบน เพื่อใช้ใน Backpropagation หา Gradient ของ Loss ที่ขึ้นกับทุก ๆ Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = input, targ = target\n",
    "def mse_grad(inp, targ): \n",
    "    inp.g = (2. * (inp.squeeze()-targ).unsqueeze(-1)/inp.shape[0])\n",
    "#     print(inp.shape, inp.g.shape)\n",
    "    assert inp.shape == inp.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp = input, out = output\n",
    "def relu_grad(inp, out): \n",
    "    inp.g = (inp>0).float() * out.g\n",
    "    assert inp.shape == inp.g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_grad(inp, out, w, b): \n",
    "    inp.g = out.g @ w.t()    \n",
    "    w.g = inp.squeeze(-1).t() @ out.g\n",
    "    b.g = out.g.sum(0)\n",
    "    assert inp.shape == inp.g.shape\n",
    "    assert w.shape == w.g.shape\n",
    "    assert b.shape == b.g.shape    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ประกาศฟังก์ชัน ให้รันทั้ง forward และ backward หา Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    # forward\n",
    "    l1 = inp @ w1 + b1\n",
    "    a1 = relu(l1)\n",
    "    l2 = a1 @ w2 + b2\n",
    "    loss = mse(l2, targ)\n",
    "    \n",
    "    # backward\n",
    "    mse_grad(l2, targ)\n",
    "    lin_grad(a1, l2, w2, b2)\n",
    "    relu_grad(l1, a1)\n",
    "    lin_grad(inp, l1, w1, b1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]), torch.Size([50000]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ได้ค่า Gradient ออกมา"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.3655), tensor(131.8213))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.g.mean(), w1.g.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะ Clone Gradient ที่ตำนวนได้เอาไว้ก่อน เอาไว้เทียบกับของ Model เวอร์ชันต่อ ๆ ไป"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1g = w1.g.clone()\n",
    "w2g = w2.g.clone()\n",
    "b1g = b1.g.clone()\n",
    "b2g = b2.g.clone()\n",
    "ig  = x_train.g.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 แอบใช้ PyTorch หา Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ใช้ PyTorch Autograd มาช่วยหา Gradient เอาไว้เช็คความถูกต้อง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt2 = x_train.clone().requires_grad_(True)\n",
    "w12 = w1.clone().requires_grad_(True)\n",
    "w22 = w2.clone().requires_grad_(True)\n",
    "b12 = b1.clone().requires_grad_(True)\n",
    "b22 = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    # forward pass:\n",
    "    l1 = inp @ w12 + b12\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w22 + b22\n",
    "    # we don't actually need the loss in backward!\n",
    "    return mse(out, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46558.8945, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = forward(xt2, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เรียก backward ให้ PyTorch ทำ Backpropagation กับทุกตัวแปรที่ requires_grad=True ด้านบน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เทสว่า Gradient จาก PyTorch ตรงกับที่เรา Clone ไว้หรือไม่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w22.grad, w2g)\n",
    "test_near(b22.grad, b2g)\n",
    "test_near(w12.grad, w1g)\n",
    "test_near(b12.grad, b1g)\n",
    "test_near(xt2.grad, ig )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เทสผ่าน แสดงว่า Gradient ที่เราคำนวนถูกต้อง ตรงกับที่ PyTorch Autograd ช่วยคำนวน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Refactor Model to Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะมา Refactor โค้ด ด้านบน แต่ละส่วน Layer, Activation Function, Loss Function ให้เป็น Class ของตัวเอง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu():\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.)\n",
    "        return self.out\n",
    "    def backward(self):\n",
    "        self.inp.g = (self.inp>0).float()*self.out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin():\n",
    "    def __init__(self, w, b): self.w, self.b = w, b\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp @ self.w + self.b\n",
    "        return self.out\n",
    "    def backward(self):\n",
    "        self.inp.g = self.out.g @ self.w.t()\n",
    "        self.w.g = self.inp.squeeze(-1).t() @ self.out.g\n",
    "        self.b.g = self.out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse():\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp, self.targ = inp, targ\n",
    "        self.out = (inp.squeeze() - targ).pow(2).mean()\n",
    "        return self.out\n",
    "    def backward(self):\n",
    "        self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.inp.shape[0]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะสร้างโมเดล Neural Networks 2 Hidden Layer จาก Class ที่เราเพิ่ง Refactor ด้านบน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
    "        self.loss = Mse()\n",
    "    def __call__(self, x, targ):\n",
    "#         set_trace()\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    def backward(self):\n",
    "        self.loss.backward()        \n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เคลียร์ Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None]*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะใช้ ข้อมูลเดิม, Weight และ Bias เดิม รันผ่านโมเดล แล้ว Backpropagate เพื่อเช็ค Gradient ว่าโมเดลทำงานได้ถูกต้องหรือไม่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Model(w1, b1, w2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46558.8945)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model(x_train, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เรียก backward ให้โมเดล ทำการ Backpropagation หา Gradient จาก Layer หลังสุดมาหน้าสุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.3655), tensor(131.8213))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.g.mean(), w1.g.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เทสว่าถูกต้อง ตรงกับที่คำนวนไว้ตอนแรก "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถูกต้อง ได้ค่าตรงกัน ไม่มี Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Refactor Class to Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ใน Class ต่าง ๆ เราจะเห็นโค้ดซ้ำ ๆ กันอยู่ เช่น โค้ดสำหรับ cache ค่า input เราสามารถ Refactor โค้ดเหล่านี้ เป็น Abstract Class ชื่อ Module ขึ้นมา เพื่อให้ Layer ชนิดต่าง ๆ มา Inherit ได้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module():\n",
    "    def __call__(self, *args):\n",
    "        self.args = args\n",
    "        self.out = self.forward(*args)\n",
    "        return self.out\n",
    "    def forward(self, inp):\n",
    "        raise Exception('Not implemented')\n",
    "    def backward(self):\n",
    "        self.bwd(self.out, *self.args)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu(Module):\n",
    "    def forward(self, inp):\n",
    "        return inp.clamp_min(0.)\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = (inp>0).float()*out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lin(Module):\n",
    "    def __init__(self, w, b):\n",
    "        self.w, self.b = w, b\n",
    "    def forward(self, inp):\n",
    "        return inp @ self.w + self.b\n",
    "    def bwd(self, out, inp):\n",
    "        inp.g = out.g @ self.w.t()\n",
    "        self.w.g = inp.squeeze(-1).t() @ out.g\n",
    "        self.b.g = out.g.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mse(Module):\n",
    "    def forward(self, inp, targ):\n",
    "        return (inp.squeeze() - targ).pow(2).mean()\n",
    "    def bwd(self, out, inp, targ):\n",
    "        inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะสร้างโมเดล Neural Networks 2 Hidden Layer จาก Module ที่เราเพิ่ง Refactor ด้านบน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เราจะใช้ ข้อมูลเดิม, Weight และ Bias เดิม รันผ่านโมเดล แล้ว Backpropagate เพื่อเช็ค Gradient ว่าโมเดลทำงานได้ถูกต้องหรือไม่"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers = [Lin(w1, b1), Relu(), Lin(w2, b2)]\n",
    "        self.loss = Mse()\n",
    "    def __call__(self, x, targ):\n",
    "#         set_trace()\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return self.loss(x, targ)\n",
    "    def backward(self):\n",
    "        self.loss.backward()        \n",
    "        for l in reversed(self.layers):\n",
    "            l.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เคลียร์ Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1.g, b1.g, w2.g, b2.g = [None] * 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้างโมเดล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เรียก backward ให้โมเดล ทำการ Backpropagation หา Gradient จาก Layer หลังสุดมาหน้าสุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เทสว่าถูกต้อง ตรงกับที่คำนวนไว้ตอนแรก "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(w2g, w2.g)\n",
    "test_near(b2g, b2.g)\n",
    "test_near(w1g, w1.g)\n",
    "test_near(b1g, b1.g)\n",
    "test_near(ig, x_train.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ถูกต้อง ได้ค่าตรงกัน ไม่มี Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. สรุป"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. เราได้เรียนรู้การสร้าง 2 Layers Deep Neural Networks ตั้งแต่เริ่มต้น คูณเมตริกซ์, Activation Function, Loss Function, ไปจน Backpropagation, หา Gradient ส่วนเรื่องการเทรนโมเดล ปรับ Hyperparameter เราจะอธิบายต่อไป\n",
    "1. เราได้ Refactor โค้ด นำมาสร้างเป็น Class, เป็น Module เพื่อเอาไว้ Reuse นำไปใช้สร้างโมเดล Deep Neural Networks ที่ซับซ้อนขึ้นอีก ในอนาคต\n",
    "1. เราได้ Initialize กำหนดค่าเริ่มต้นของ Weight และ Bias ด้วยค่า Random แบบ Normal Distribution (Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://course.fast.ai/videos/?lesson=8\n",
    "* https://pytorch.org/docs/stable/nn.html#torch.nn.Module\n",
    "* https://deeplearning.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
