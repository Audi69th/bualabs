{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "16j-callback-lr-finder.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmtceCFGGajO",
        "colab_type": "text"
      },
      "source": [
        "จาก ep ก่อนที่เราได้เรียนรู้ความสำคัญของ Hyperparameter ของการเทรน Machine Learning ที่ชื่อ [Learning Rate](https://www.bualabs.com/archives/618/learning-rate-deep-learning-how-to-hyperparameter-tuning-ep-1/) ถ้าเรากำหนดค่า Learning น้อยไปก็ทำให้เทรนได้ช้า แต่ถ้ามากเกินไปก็ทำให้ไม่ Converge\n",
        "\n",
        "แล้วเราจะทราบได้อย่างไร ว่า Learning Rate เท่าไร เป็นค่าที่ดีที่สุดในการเทรน Deep Neural Network ของเรา\n",
        "\n",
        "เราจะสอนวิธีหา Learning Rate ที่ดีที่สุด ที่ดีที่สุด หมายถึง Learning Rate ที่มากที่สุด ที่ยังไม่ทำให้เกิดการไม่ Converge ด้วยอัลกอริทึมง่าย ๆ ตรงตัว คือ การทดลองเทรน แล้วเพิ่ม Learning Rate ขึ้นไปเรื่อย ๆ แล้วเช็ค Loss จนกว่า Loss จะเพิ่มมากจนผิดปกติ เรียกว่า LR Finder (Learning Rate Finder) โดยใช้ [LR Finder Callback เริ่มที่หัวข้อ 6](#6.-Callbacks)\n",
        "\n",
        "เมื่อเราได้ข้อมูล ความสัมพันธ์ระหว่าง Learning Rate กับ Loss ของโมเดล Deep Neural Network ของเรามาเรียบร้อยแล้ว เราจะนำมาพล็อตกราฟ เพื่อวิเคราะห์หา Learning Rate ที่ดีที่สุดต่อไป"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3drCJqK97wZ5",
        "colab_type": "text"
      },
      "source": [
        "# 0. Magic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As6QTNhIo7LC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJdvUE9d7y7L",
        "colab_type": "text"
      },
      "source": [
        "# 1. Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0k1vCN_7zFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "from torch.nn import *\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from fastai import datasets\n",
        "from fastai.metrics import accuracy\n",
        "import pickle, gzip, math, torch, re\n",
        "from IPython.core.debugger import set_trace\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYLQCdBq7zNn",
        "colab_type": "text"
      },
      "source": [
        "# 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-IMKGED7zmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x, self.y = x, y\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, i):\n",
        "        return self.x[i], self.y[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiPq9nOu8V-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8Jl_Ekx7zVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data():\n",
        "    path = datasets.download_data(MNIST_URL, ext='.gz')\n",
        "    with gzip.open(path, 'rb') as f:\n",
        "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
        "    return map(tensor, (x_train, y_train, x_valid, y_valid))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzugVJ09eCrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train, x_valid, y_valid = get_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d9ixSpfgUeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(x, m, s): \n",
        "    return (x-m)/s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrGYLaZU7age",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import *\n",
        "\n",
        "def listify(o):\n",
        "    if o is None: return []\n",
        "    if isinstance(o, list): return o\n",
        "    if isinstance(o, str): return [o]\n",
        "    if isinstance(o, Iterable): return list(o)\n",
        "    return [o]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykdiURV28_se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_camel_re1 = re.compile('(.)([A-Z][a-z]+)')\n",
        "_camel_re2 = re.compile('([a-z0-9])([A-Z])')\n",
        "\n",
        "def camel2snake(name):\n",
        "    s1 = re.sub(_camel_re1, r'\\1_\\2', name)\n",
        "    return re.sub(_camel_re2, r'\\1_\\2', s1).lower()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEtfU8TjgO-t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_mean, train_std = x_train.mean(), x_train.std()\n",
        "x_train = normalize(x_train, train_mean, train_std)\n",
        "x_valid = normalize(x_valid, train_mean, train_std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwr_S24teD1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nh, bs = 100, 32\n",
        "n, m = x_train.shape\n",
        "c = (y_train.max()+1).numpy()\n",
        "loss_func = F.cross_entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxDyNxD07zeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
        "train_dl, valid_dl = DataLoader(train_ds, bs), DataLoader(valid_ds, bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWKmfCFVLOmn",
        "colab_type": "text"
      },
      "source": [
        "# 3. DataBunch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B-9fQQJ-8RS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataBunch():\n",
        "    def __init__(self, train_dl, valid_dl, c=None):\n",
        "        self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c\n",
        "\n",
        "    @property\n",
        "    def train_ds(self): return self.train_dl.dataset\n",
        "\n",
        "    @property\n",
        "    def valid_ds(self): return self.valid_dl.dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3OzWeXWBzLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataBunch(train_dl, valid_dl, c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9EbSM4m8lef",
        "colab_type": "text"
      },
      "source": [
        "# 4. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtoT7A2DggGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.03\n",
        "epoch = 10\n",
        "nh = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBp4jzQoeWLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model():\n",
        "    # loss function\n",
        "    loss_func = F.cross_entropy\n",
        "    model = Sequential(Linear(m, nh), ReLU(), Linear(nh,c))\n",
        "    return model, loss_func"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXwrxHGFhqJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Learner():\n",
        "    def __init__(self, model, opt, loss_func, data):\n",
        "        self.model, self.opt, self.loss_func, self.data = model, opt, loss_func, data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5p_H1kWNd9ab",
        "colab_type": "text"
      },
      "source": [
        "# 5. Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yFdOKNfgbVB",
        "colab_type": "text"
      },
      "source": [
        "Training Loop ที่รองรับ Callback"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp3wybPiGylV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Runner():\n",
        "    def __init__(self, cbs=None, cb_funcs=None):\n",
        "        cbs = listify(cbs)\n",
        "        for cbf in listify(cb_funcs):\n",
        "            cb = cbf()\n",
        "            setattr(self, cb.name, cb)\n",
        "            cbs.append(cb)\n",
        "        self.stop, self.cbs = False, [TrainEvalCallback()]+cbs\n",
        "\n",
        "    @property\n",
        "    def opt(self):          return self.learn.opt\n",
        "    @property\n",
        "    def model(self):        return self.learn.model\n",
        "    @property\n",
        "    def loss_func(self):    return self.learn.loss_func\n",
        "    @property\n",
        "    def data(self):         return self.learn.data\n",
        "\n",
        "    def one_batch(self, xb, yb):\n",
        "        try: \n",
        "            self.xb, self.yb = xb, yb\n",
        "            self('begin_batch')\n",
        "            self.pred = self.model(xb)\n",
        "            self('after_pred')\n",
        "            self.loss = self.loss_func(self.pred, yb)\n",
        "            self('after_loss')\n",
        "            if not self.in_train: return\n",
        "            self.loss.backward()\n",
        "            self('after_backward')\n",
        "            self.opt.step()\n",
        "            self('after_step')\n",
        "            self.opt.zero_grad()\n",
        "        except CancelBatchException: self('after_cancel_batch')\n",
        "        finally: self('after_batch')\n",
        "    \n",
        "    def all_batches(self, dl):\n",
        "        self.iters = len(dl)\n",
        "        try:\n",
        "            for xb, yb in dl:\n",
        "                self.one_batch(xb, yb)\n",
        "        except CancelEpochException: self('after_cancel_epoch')\n",
        "    \n",
        "    def fit(self, epochs, learn):\n",
        "        self.epochs, self.learn, self.loss = epochs, learn, tensor(0.)\n",
        "\n",
        "        try:\n",
        "            for cb in self.cbs: cb.set_runner(self)\n",
        "            self('begin_fit')\n",
        "            for epoch in range(epochs):\n",
        "                self.epoch = epoch\n",
        "                if not self('begin_epoch'): self.all_batches(self.data.train_dl)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    if not self('begin_validate'): self.all_batches(self.data.valid_dl)\n",
        "                self('after_epoch')\n",
        "        except CancelTrainException: self('after_cancel_train')\n",
        "        finally: \n",
        "            self('after_fit')\n",
        "            self.train = None\n",
        "\n",
        "    def __call__(self, cb_name):\n",
        "        # return True = Cancel, return False = Continue (Default)\n",
        "        res = False\n",
        "        # check if at least one True return True\n",
        "        for cb in sorted(self.cbs, key=lambda x: x._order): res = res or cb(cb_name)\n",
        "        return res        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0O9bxvAG5BW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Callback():\n",
        "    _order = 0\n",
        "    def set_runner(self, run): self.run = run\n",
        "    def __getattr__(self, k): return getattr(self.run, k)\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        name = re.sub(r'Callback$', '', self.__class__.__name__)\n",
        "        return camel2snake(name or 'callback')\n",
        "    \n",
        "    def __call__(self, cb_name):\n",
        "        f = getattr(self, cb_name, None)\n",
        "        if f and f(): return True\n",
        "        return False\n",
        "\n",
        "class TrainEvalCallback(Callback):\n",
        "    def begin_fit(self):\n",
        "        self.run.n_epochs = 0.\n",
        "        self.run.n_iter = 0\n",
        "    \n",
        "    def begin_epoch(self):\n",
        "        self.run.n_epochs = self.epoch  \n",
        "        self.model.train()\n",
        "        self.run.in_train=True\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        self.run.n_epochs += 1./self.iters\n",
        "        self.run.n_iter += 1\n",
        "\n",
        "    def begin_validate(self):\n",
        "        self.model.eval()\n",
        "        self.run.in_train=False    \n",
        "           \n",
        "class CancelTrainException(Exception): pass\n",
        "class CancelEpochException(Exception): pass\n",
        "class CancelBatchException(Exception): pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOSJFVjTbMPC",
        "colab_type": "text"
      },
      "source": [
        "เราจะเพิ่ม Method Recorder.plot สำหรับพล็อตกราฟ ความสัมพันธ์ระหว่าง Learning Rate กับ Loss \n",
        "\n",
        "และเพิ่ม Parameter ให้กับ plot_lr, plot_loss ดังนี้ \n",
        "\n",
        "* pgid = Parameter Group ID เราแบ่ง Parameter ของโมเดลออกเป็น 3 กลุ่มที่มี Learning Rate แตกต่างกัน จะอธิบายต่อไป ตอนนี้ให้ใช้ -1 หมายถึงสุดท้าย\n",
        "* skip_last = จำนวน Iteration ที่ข้ามไม่ต้องพล็อตกราฟ นับจากท้ายสุด เนื่องจาก Iteration ท้าย ๆ มักจะ Loss มาก จนทำให้กราฟดูยาก"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5zJVLbeqK8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Recorder(Callback):\n",
        "    def begin_fit(self): \n",
        "        self.lrs = [[] for _ in self.opt.param_groups]\n",
        "        self.losses = []\n",
        "\n",
        "    def after_batch(self):\n",
        "        if not self.in_train: return\n",
        "        for pg, lr in zip(self.opt.param_groups, self.lrs): lr.append(pg['lr'])\n",
        "        self.losses.append(self.loss.detach().cpu())\n",
        "    \n",
        "    def plot_lr(self, pgid=-1): plt.plot(self.lrs[pgid])\n",
        "    def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])\n",
        "    def plot(self, skip_last=0, pgid=-1):\n",
        "        losses = [o.item() for o in self.losses]\n",
        "        lrs = self.lrs[pgid]\n",
        "        n = len(losses)-skip_last\n",
        "        plt.xscale('log')\n",
        "        plt.plot(lrs[:n], losses[:n])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-kacUKlJk6j",
        "colab_type": "text"
      },
      "source": [
        "# 6. Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArRWTAm6tWl0",
        "colab_type": "text"
      },
      "source": [
        "## 6.1 LR_Find (Learning Rate Finder Callback)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKDri3fGJlx2",
        "colab_type": "text"
      },
      "source": [
        "เราสร้าง Callback ที่ในระหว่างการเทรน จะเพิ่มค่า lr (Learning Rate) ในทุก ๆ Batch แบบ Exponential โดยเริ่มที่ min_lr แต่ไม่เกิน max_lr\n",
        "\n",
        "และหลังจากเทรนจบ Batch ลองเช็คค่า Loss เปรียบเทียบว่ามากกว่า 10 เท่า ของ ค่า Loss ที่น้อยที่สุด ที่เคยได้หรือไม่ ถ้าใช่ก็แปลว่า Learning Rate มากเกินไปแล้ว จบการเทรนได้ แต่ถ้าไม่ใช่ก็เก็บค่า Loss ที่ดีที่สุดไว้ แล้วเทรนต่อ\n",
        "\n",
        "max_iter = เราจะเทรนกี่ Iteration (Mini-Batch), min_lr = Learning Rate ขั้นต่ำ, max_lr = เพดาน Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvtme4ONJmMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LR_Find(Callback):\n",
        "    _order = 1\n",
        "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):\n",
        "        self.max_iter, self.min_lr, self.max_lr = max_iter, min_lr, max_lr\n",
        "        self.best_loss = 1e9\n",
        "\n",
        "    def begin_batch(self):\n",
        "        if not self.in_train: return\n",
        "        pos = self.n_iter/self.max_iter\n",
        "        lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
        "        for pg in self.opt.param_groups: pg['lr'] = lr\n",
        "    \n",
        "    def after_loss(self):\n",
        "        if self.n_iter>=self.max_iter or self.loss > self.best_loss*10: \n",
        "            raise CancelTrainException()\n",
        "        if self.loss < self.best_loss: self.best_loss = self.loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGu2ipG4zIuO",
        "colab_type": "text"
      },
      "source": [
        "ลองเราลองเทรนด้วย Callback 2 ตัว คือ LR_Find เพื่อหา Learning Rate และ และ Recorder เพื่อบันทึกค่า Learning Rate และ Loss ในระหว่างการเทรน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBrIkNAxiZV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model, loss_func = get_model()\n",
        "opt = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "learn = Learner(model, opt, loss_func, data)\n",
        "\n",
        "run = Runner(cb_funcs=[LR_Find, Recorder])\n",
        "run.fit(5, learn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dphC63CUJQqs",
        "colab_type": "text"
      },
      "source": [
        "พล็อตกราฟ Iteration, Learning Rate ดูว่าในแต่ละ Iteration, Learning Rate นั้น เพิ่มแบบ Exponential ในช่วงระหว่าง min_lr, max_lr"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_W7XVY5JM6L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run.recorder.plot_lr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4uehmV5XZYN",
        "colab_type": "text"
      },
      "source": [
        "พล็อตกราฟความสัมพันธ์ระหว่าง Learing Rate, Loss ดูว่า Loss จะลดลงเร็วขึ้นเรื่อย ๆ เมื่อเราเพิ่ม Learning Rate แต่จะลดลงไปถึงค่าหนึ่ง แล้วจะพุ่งขึ้นอย่างรวดเร็ว"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSBiWuJ3Jeau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "run.recorder.plot(skip_last=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51UHdTDeV5fS",
        "colab_type": "text"
      },
      "source": [
        "ให้เราเลือก Learning Rate ที่ต่ำสุดก่อนที่ Loss จะพุ่งขึ้น 1e-1 ย้อนไป 10 เท่า ในที่นี้คือ lr = 10e-2\n",
        "\n",
        "เมื่อเราลองเทรนด้วย lr = 10e-2 แล้ว เราสามารถลอง +- 10 เท่าได้ เป็น 10e-1, 10e-3 เพื่อเปรียบเทียบได้"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dV_N1pcIivIt",
        "colab_type": "text"
      },
      "source": [
        "# Credit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8k7cjX2jXei",
        "colab_type": "text"
      },
      "source": [
        "* https://course.fast.ai/videos/?lesson=10\n",
        "* http://yann.lecun.com/exdb/mnist/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdNNwoXYMVgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}