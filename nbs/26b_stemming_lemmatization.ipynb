{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "26b_stemming_lemmatization.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siKk7nqixfr5",
        "colab_type": "text"
      },
      "source": [
        "ตามหลักตามไวยากรณ์ภาษาอังกฤษ คำหนึ่งคำจะแปรไปได้หลายรูปแบบ เช่น organize, organizes, organized, organizing นอกจากนั้นคำยังสามารถแปลงเป็นกลุ่มคำ ที่มาจากรากศัพท์เดียวกันได้อีกหลายรูปแบบ เช่น democracy, democratic, democratization ในบางเคสเราต้องการค้นหาคำบางคำในกลุ่ม แล้วอยากให้ได้ผลลัพธ์ครอบคลุมทุกคำทั้งกลุ่ม แล้วเราจะทำอย่างไร\n",
        "\n",
        "เราต้องการกระบวนการที่จะลดรูปคำศัพท์ แปลง word ให้อยู่ในรูปฟอร์มพื้นฐาน เช่น \n",
        "\n",
        "* am, are, is ==> be\n",
        "* car, cars, car's, cars' ==> car\n",
        "\n",
        "ตัวอย่าง เมื่อนำมาใช้กับข้อความทั้งประโยค เช่น \n",
        "\n",
        "* the boy's cars are different colors ==>\n",
        "the boy car be differ color\n",
        "\n",
        "วิธีที่เป็นที่นิยมมี 2 อย่าง เรียกว่า Lemmatization และ Stemming "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICk37RZSUJ28",
        "colab_type": "text"
      },
      "source": [
        "## Stemming คืออะไร \n",
        "\n",
        "Stemming คือ กระบวนตัดส่วนท้ายของคำ แบบหยาบ ๆ ด้วย Heuristic ซึ่งได้ผลดีพอควร สำหรับคำในภาษาอังกฤษส่วนใหญ่ แต่ไม่ทุกคำ Stemming ทำให้ลดฟอร์มลง เหลือแต่ส่วนหน้าของคำที่เหมือน ๆ กันในคำกลุ่มเดียวกัน\n",
        "\n",
        "## Lemmatization คืออะไร \n",
        "\n",
        "Lemmatization คือ กระบวนการในการแปลง Word ด้วยรายการคำศัพท์ใน Dictionary, การวิเคราะห์หลักไวยกรณ์ของภาษาอย่างเหมาะสม ในการแปรคำ ผันคำ เพื่อกำจัด Inflection ของคำ เช่น เพศ, Tense, เสียง, อารมณ์, จำนวน, etc. ส่วนใหญ่จะตัดส่วนท้าย ให้เหลือแต่รูปฟอร์มพื้นฐาน เป็นคำใน Dictionary เรียกว่า Lemma\n",
        "\n",
        "ตัวอย่างเช่น saw ถ้าใช้ Stemming จะทำได้ดีที่สุดแค่ s แต่ถ้าใช้ Lemmatization จะได้ see หรือ saw ขึ้นอยู่กับว่าเป็น Noun หรือ Verb \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xi-7-eTKS_RC",
        "colab_type": "text"
      },
      "source": [
        "# 0. Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A54_hDQS-YE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ! pip install -U spacy\n",
        "# ! pip install -U spacy-lookups-data\n",
        "# ! python -m spacy download en_core_web_sm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASzTS9s8zmx5",
        "colab_type": "text"
      },
      "source": [
        "# 1. Stemming and Lemmatization using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yved9ZyCLasz",
        "colab_type": "text"
      },
      "source": [
        "ความสามารถในการ Stemming และ Lemmatization นั้นขึ้นอยู่กับ Library ในเคสนี้ เราจะใช้ Library ชื่อ NLTK ซึ่งเป็น Library พื้นฐาน ที่นิยมใช้ในงาน NLP ของ Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxFUoH6fphOK",
        "colab_type": "code",
        "outputId": "9853a1cf-3885-4c14-fa7f-3bc93ad7cda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8crLhESMLn0b",
        "colab_type": "text"
      },
      "source": [
        "Import Package สำหรับ Stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qppO1Rv3snY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import stem"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZaBpGiuLtgs",
        "colab_type": "text"
      },
      "source": [
        "สร้าง WordNetLemmatizer และ PorterStemmer ซึ่งเป็น Stemmer และ Lemmatizer ภาษาอังกฤษ ที่คนนิยมใช้ ทำงานได้ดี "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IyXByrAsppA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wnl = stem.WordNetLemmatizer()\n",
        "porter = stem.porter.PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7GdEFU7MEJY",
        "colab_type": "text"
      },
      "source": [
        "ประกาศฟังก์ชัน สำหรับ print คำเปรียบเทียบ Before, After"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpHZuixStKsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_lemma_stem(word_list):\n",
        "    for word in word_list:\n",
        "        print(f'{word:12} ==> {wnl.lemmatize(word):12}\\t{porter.stem(word):12}')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgqR3xflMK20",
        "colab_type": "text"
      },
      "source": [
        "ตัวอย่างการใช้งาน ก่อนแปลง หลังแปลงด้วย Lemmatizer และ Stemmer ด้วย NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i95_ml6syJC",
        "colab_type": "code",
        "outputId": "3a671983-3924-4895-b4e5-718c829d950f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "word_list = ['foot', 'feet', 'foots', 'footing']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "foot         ==> foot        \tfoot        \n",
            "feet         ==> foot        \tfeet        \n",
            "foots        ==> foot        \tfoot        \n",
            "footing      ==> footing     \tfoot        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM7kKZkAs4Fu",
        "colab_type": "code",
        "outputId": "80ac5f0f-3ce1-4f07-fdef-a37b17e8e536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "word_list = ['fly', 'flies', 'flying', 'flew', 'flown']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fly          ==> fly         \tfli         \n",
            "flies        ==> fly         \tfli         \n",
            "flying       ==> flying      \tfli         \n",
            "flew         ==> flew        \tflew        \n",
            "flown        ==> flown       \tflown       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlURbqXCs-R6",
        "colab_type": "code",
        "outputId": "f00608e3-295a-4d4f-84f7-11691e7cb9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "word_list = ['organize', 'organized', 'organizing']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "organize     ==> organize    \torgan       \n",
            "organized    ==> organized   \torgan       \n",
            "organizing   ==> organizing  \torgan       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rUF4__GtECb",
        "colab_type": "code",
        "outputId": "ac761fc9-f5db-43a7-bd01-eb58fedd2eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "word_list = ['universe', 'university', 'universal']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "universe     ==> universe    \tunivers     \n",
            "university   ==> university  \tunivers     \n",
            "universal    ==> universal   \tunivers     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMO3t3mxtYYk",
        "colab_type": "code",
        "outputId": "2b665eef-cadd-49bf-b142-2be8ba108935",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "word_list = ['eat', 'eats', 'ate', 'eaten']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eat          ==> eat         \teat         \n",
            "eats         ==> eats        \teat         \n",
            "ate          ==> ate         \tate         \n",
            "eaten        ==> eaten       \teaten       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fmwfblutZ3L",
        "colab_type": "code",
        "outputId": "5410e74f-5868-4cbd-c912-d234f52a2d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "word_list = ['win', 'wins', 'won', 'winning']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "win          ==> win         \twin         \n",
            "wins         ==> win         \twin         \n",
            "won          ==> won         \twon         \n",
            "winning      ==> winning     \twin         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0twta0dItvrA",
        "colab_type": "code",
        "outputId": "11355814-00bf-45a5-a054-5fb1778cda8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "word_list = ['cat     ', 'category', 'categorize']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat          ==> cat         \tcat         \n",
            "category     ==> category    \tcategori    \n",
            "categorize   ==> categorize  \tcategor     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrilM3rrt9bj",
        "colab_type": "code",
        "outputId": "b89fb337-eac3-4bcd-9aa0-1aa978f658f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "word_list = ['is ', 'am ', 'are', 'be ']\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is           ==> is          \tis          \n",
            "am           ==> am          \tam          \n",
            "are          ==> are         \tare         \n",
            "be           ==> be          \tbe          \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZO0p6OlOMf0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c73cf86b-bfde-46c3-c008-41231f6c7bc7"
      },
      "source": [
        "word_list = \"The formatting operations described here exhibit a variety of quirks that lead to a number of common errors.\".split()\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The          ==> The         \tthe         \n",
            "formatting   ==> formatting  \tformat      \n",
            "operations   ==> operation   \toper        \n",
            "described    ==> described   \tdescrib     \n",
            "here         ==> here        \there        \n",
            "exhibit      ==> exhibit     \texhibit     \n",
            "a            ==> a           \ta           \n",
            "variety      ==> variety     \tvarieti     \n",
            "of           ==> of          \tof          \n",
            "quirks       ==> quirk       \tquirk       \n",
            "that         ==> that        \tthat        \n",
            "lead         ==> lead        \tlead        \n",
            "to           ==> to          \tto          \n",
            "a            ==> a           \ta           \n",
            "number       ==> number      \tnumber      \n",
            "of           ==> of          \tof          \n",
            "common       ==> common      \tcommon      \n",
            "errors.      ==> errors.     \terrors.     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XrbUoetR2LO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "33071969-72ec-4593-82b4-5ef8877f8564"
      },
      "source": [
        "word_list = \"Fear is not real. The only place that fear can exist is in our thoughts of the future. It is a product of our imagination, causing us to fear things that do not at present and may not ever exist. That is near insanity. Do not misunderstand me danger is very real but fear is a choice.\".split()\n",
        "print_lemma_stem(word_list)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fear         ==> Fear        \tfear        \n",
            "is           ==> is          \tis          \n",
            "not          ==> not         \tnot         \n",
            "real.        ==> real.       \treal.       \n",
            "The          ==> The         \tthe         \n",
            "only         ==> only        \tonli        \n",
            "place        ==> place       \tplace       \n",
            "that         ==> that        \tthat        \n",
            "fear         ==> fear        \tfear        \n",
            "can          ==> can         \tcan         \n",
            "exist        ==> exist       \texist       \n",
            "is           ==> is          \tis          \n",
            "in           ==> in          \tin          \n",
            "our          ==> our         \tour         \n",
            "thoughts     ==> thought     \tthought     \n",
            "of           ==> of          \tof          \n",
            "the          ==> the         \tthe         \n",
            "future.      ==> future.     \tfuture.     \n",
            "It           ==> It          \tIt          \n",
            "is           ==> is          \tis          \n",
            "a            ==> a           \ta           \n",
            "product      ==> product     \tproduct     \n",
            "of           ==> of          \tof          \n",
            "our          ==> our         \tour         \n",
            "imagination, ==> imagination,\timagination,\n",
            "causing      ==> causing     \tcaus        \n",
            "us           ==> u           \tus          \n",
            "to           ==> to          \tto          \n",
            "fear         ==> fear        \tfear        \n",
            "things       ==> thing       \tthing       \n",
            "that         ==> that        \tthat        \n",
            "do           ==> do          \tdo          \n",
            "not          ==> not         \tnot         \n",
            "at           ==> at          \tat          \n",
            "present      ==> present     \tpresent     \n",
            "and          ==> and         \tand         \n",
            "may          ==> may         \tmay         \n",
            "not          ==> not         \tnot         \n",
            "ever         ==> ever        \tever        \n",
            "exist.       ==> exist.      \texist.      \n",
            "That         ==> That        \tthat        \n",
            "is           ==> is          \tis          \n",
            "near         ==> near        \tnear        \n",
            "insanity.    ==> insanity.   \tinsanity.   \n",
            "Do           ==> Do          \tDo          \n",
            "not          ==> not         \tnot         \n",
            "misunderstand ==> misunderstand\tmisunderstand\n",
            "me           ==> me          \tme          \n",
            "danger       ==> danger      \tdanger      \n",
            "is           ==> is          \tis          \n",
            "very         ==> very        \tveri        \n",
            "real         ==> real        \treal        \n",
            "but          ==> but         \tbut         \n",
            "fear         ==> fear        \tfear        \n",
            "is           ==> is          \tis          \n",
            "a            ==> a           \ta           \n",
            "choice.      ==> choice.     \tchoice.     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4HDf5p2uqf9",
        "colab_type": "text"
      },
      "source": [
        "# 2. Lemmatization using Spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTt0YJ_CQWY7",
        "colab_type": "text"
      },
      "source": [
        "Spacy เป็น Library ที่ใหม่กว่า Optimize กว่า ทำงานได้เร็วกว่า แต่ฟังก์ชันการใช้งานอาจจะไม่ครอบคลุมเหมือนตัวข้างบน เช่น Spacy ไม่มี Stemming มีแต่ Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPVmXODzzsAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.lemmatizer import Lemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHxbQKUGRMu9",
        "colab_type": "text"
      },
      "source": [
        "โหลดโมเดล ภาษาอังกฤษ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnagEYOHzxSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzNpPP_dSD0f",
        "colab_type": "text"
      },
      "source": [
        "ประกาศฟังก์ชัน สำหรับ print คำเปรียบเทียบ Before, After"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL_adbtyzy3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_lemma(sentence):\n",
        "    sentence = nlp(sentence)\n",
        "    for word in sentence:\n",
        "        print(f'{word.text:12} \\t==> {word.lemma_:12}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQUlrtG6SQVe",
        "colab_type": "text"
      },
      "source": [
        "ตัวอย่างการใช้งาน ก่อนแปลง หลังแปลงด้วย Lemmatizer ด้วย Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XKAdAoOz07a",
        "colab_type": "code",
        "outputId": "91b2f911-90e8-4e7e-ede0-d480dfaee6b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print_lemma('car cars')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "car          \t==> car         \n",
            "cars         \t==> car         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "719m5T7Pz7wd",
        "colab_type": "code",
        "outputId": "2d97d09c-55f0-49f3-edc0-25450a29b00d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print_lemma('fly flies flying flew flown')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fly          \t==> fly         \n",
            "flies        \t==> fly         \n",
            "flying       \t==> fly         \n",
            "flew         \t==> fly         \n",
            "flown        \t==> fly         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCOCBZJxz9b5",
        "colab_type": "code",
        "outputId": "a552594d-ddc3-4521-989a-63ace5824f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print_lemma('organize organized organizes organizing organization organizations')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "organize     \t==> organize    \n",
            "organized    \t==> organize    \n",
            "organizes    \t==> organize    \n",
            "organizing   \t==> organize    \n",
            "organization \t==> organization\n",
            "organizations \t==> organization\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waz0bgthz_Qk",
        "colab_type": "code",
        "outputId": "3ee16f2a-a042-4a6c-8b17-f3ec0cde4580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print_lemma('universe university universal universalist')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "universe     \t==> universe    \n",
            "university   \t==> university  \n",
            "universal    \t==> universal   \n",
            "universalist \t==> universalist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3w63nUmT_kE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e54c9463-f1de-45f7-f1c9-ab990cffa4dc"
      },
      "source": [
        "print_lemma('The formatting operations described here exhibit a variety of quirks that lead to a number of common errors.')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The          \t==> the         \n",
            "formatting   \t==> format      \n",
            "operations   \t==> operation   \n",
            "described    \t==> describe    \n",
            "here         \t==> here        \n",
            "exhibit      \t==> exhibit     \n",
            "a            \t==> a           \n",
            "variety      \t==> variety     \n",
            "of           \t==> of          \n",
            "quirks       \t==> quirk       \n",
            "that         \t==> that        \n",
            "lead         \t==> lead        \n",
            "to           \t==> to          \n",
            "a            \t==> a           \n",
            "number       \t==> number      \n",
            "of           \t==> of          \n",
            "common       \t==> common      \n",
            "errors       \t==> error       \n",
            ".            \t==> .           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO4CrZfsRhGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a6a7e099-1f93-47bf-ba88-92d8dc4d65fb"
      },
      "source": [
        "print_lemma('Fear is not real. The only place that fear can exist is in our thoughts of the future. It is a product of our imagination, causing us to fear things that do not at present and may not ever exist. That is near insanity. Do not misunderstand me danger is very real but fear is a choice.')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fear         \t==> fear        \n",
            "is           \t==> be          \n",
            "not          \t==> not         \n",
            "real         \t==> real        \n",
            ".            \t==> .           \n",
            "The          \t==> the         \n",
            "only         \t==> only        \n",
            "place        \t==> place       \n",
            "that         \t==> that        \n",
            "fear         \t==> fear        \n",
            "can          \t==> can         \n",
            "exist        \t==> exist       \n",
            "is           \t==> be          \n",
            "in           \t==> in          \n",
            "our          \t==> -PRON-      \n",
            "thoughts     \t==> thought     \n",
            "of           \t==> of          \n",
            "the          \t==> the         \n",
            "future       \t==> future      \n",
            ".            \t==> .           \n",
            "It           \t==> -PRON-      \n",
            "is           \t==> be          \n",
            "a            \t==> a           \n",
            "product      \t==> product     \n",
            "of           \t==> of          \n",
            "our          \t==> -PRON-      \n",
            "imagination  \t==> imagination \n",
            ",            \t==> ,           \n",
            "causing      \t==> cause       \n",
            "us           \t==> -PRON-      \n",
            "to           \t==> to          \n",
            "fear         \t==> fear        \n",
            "things       \t==> thing       \n",
            "that         \t==> that        \n",
            "do           \t==> do          \n",
            "not          \t==> not         \n",
            "at           \t==> at          \n",
            "present      \t==> present     \n",
            "and          \t==> and         \n",
            "may          \t==> may         \n",
            "not          \t==> not         \n",
            "ever         \t==> ever        \n",
            "exist        \t==> exist       \n",
            ".            \t==> .           \n",
            "That         \t==> that        \n",
            "is           \t==> be          \n",
            "near         \t==> near        \n",
            "insanity     \t==> insanity    \n",
            ".            \t==> .           \n",
            "Do           \t==> Do          \n",
            "not          \t==> not         \n",
            "misunderstand \t==> misunderstand\n",
            "me           \t==> -PRON-      \n",
            "danger       \t==> danger      \n",
            "is           \t==> be          \n",
            "very         \t==> very        \n",
            "real         \t==> real        \n",
            "but          \t==> but         \n",
            "fear         \t==> fear        \n",
            "is           \t==> be          \n",
            "a            \t==> a           \n",
            "choice       \t==> choice      \n",
            ".            \t==> .           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVayLtqS0Asz",
        "colab_type": "text"
      },
      "source": [
        "# Credit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGAspwyM1FVh",
        "colab_type": "text"
      },
      "source": [
        "* https://github.com/fastai/course-nlp\n",
        "* https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
        "* https://www.nltk.org/api/nltk.stem.html\n",
        "* https://spacy.io/api/lemmatizer\n",
        "* https://www.imdb.com/title/tt1815862/characters/nm0000226"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ok6fE51Lfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}